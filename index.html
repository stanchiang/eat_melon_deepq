<!DOCTYPE html>
<!-- saved from url=(0066)https://cs.stanford.edu/people/karpathy/convnetjs/demo/rldemo.html -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <!-- <script src="./rldemo_files/convnet.js"></script> -->
  <script src="./rldemo_files/util.js"></script>
  <script src="./rldemo_files/vis.js"></script>
  <script src="./rldemo_files/deepqlearn.js"></script>
  
  <style>
  #wrap {
    width:800px;
    margin-left: auto;
    margin-right: auto;
  }
  h2 {
    text-align: center;
  }
  h1 {
    font-size: 16px;
    color: #333;
    background-color: #DDD;
    border-bottom: 1px #999 solid;
    text-align: center;
  }
  body {
    font-family: Arial, "Helvetica Neue", Helvetica, sans-serif;
  }
  </style>


  <script type="application/javascript">
    var return_v = false;
    var v_val = 0.0;

    function gaussRandom() {
      if (return_v) {
        return_v = false;
        return v_val;
      }
      var u = 2 * Math.random() - 1;
      var v = 2 * Math.random() - 1;
      var r = u * u + v * v;
      if (r == 0 || r > 1) return gaussRandom();
      var c = Math.sqrt(-2 * Math.log(r) / r);
      v_val = v * c; // cache this
      return_v = true;
      return u * c;
    }

    function randf(a, b) {
      return Math.random() * (b - a) + a;
    }

    function randi(a, b) {
      return Math.floor(Math.random() * (b - a) + a);
    }

    function randn(mu, std) {
      return mu + gaussRandom() * std;
    }

    var canvas, ctx;
    
    // A 2D vector utility
    var Vec = function(x, y) {
      this.x = x;
      this.y = y;
    }
    Vec.prototype = {
      
      // utilities
      dist_from: function(v) { return Math.sqrt(Math.pow(this.x-v.x,2) + Math.pow(this.y-v.y,2)); },
      length: function() { return Math.sqrt(Math.pow(this.x,2) + Math.pow(this.y,2)); },
      
      // new vector returning operations
      add: function(v) { return new Vec(this.x + v.x, this.y + v.y); },
      sub: function(v) { return new Vec(this.x - v.x, this.y - v.y); },
      rotate: function(a) {  // CLOCKWISE
        return new Vec(this.x * Math.cos(a) + this.y * Math.sin(a),
                       -this.x * Math.sin(a) + this.y * Math.cos(a));
      },
      
      // in place operations
      scale: function(s) { this.x *= s; this.y *= s; },
      normalize: function() { var d = this.length(); this.scale(1.0/d); }
    }

  
    var poop = new Image();
    var watermelon = new Image();
    var eater = new Image();
    // Emojis from http://icons16x16.com/?fuseaction=folders&foldername=iOS%20emoji
    poop.src = './rldemo_files/images/090-PILE-OF-POO.png'; //24 by 24
    watermelon.src = './rldemo_files/images/525-WATERMELON.png';
    eater.src = './rldemo_files/images/048-SMILING-FACE-WITH-HORNS.png';  

    xspd = 0.0;
    yspd = 0.0; 
    emin = .05;
    gamma = 0.7;
    dig_water = 5;
    dig_poop = -6;
    reward_aw = 1;
    reward_f = 0.1;
    items_total = 4;
       
    function myRangeX(value) {xspd = value/100};
    function myRangeY(value) {yspd = value/100};
    function myRangeE(value) {emin = value/100}; //Epsilon
    function myRangeG(value) {gamma = value/100}; //gamma
    function myRangeW(value) {dig_water = value/1}; //watermelon
    function myRangeP(value) {dig_poop = value/1}; //poop
    function myRangeAW(value) {reward_aw = value/1}; //poop
    function myRangeF(value) {reward_f = value/10}; //poop


    var num_inputs = 27; // 9 eyes, each sees 3 numbers (wall, green, red thing proximity)
    var num_actions = 5; // 5 possible angles agent can turn
    var temporal_window = 1; // amount of temporal memory. 0 = agent lives in-the-moment :)
    var network_size = num_inputs*temporal_window + num_actions*temporal_window + num_inputs;

    // the value function network computes a value of taking any of the possible actions
    // given an input state. Here we specify one explicitly the hard way
    // but user could also equivalently instead use opt.hidden_layer_sizes = [20,20]
    // to just insert simple relu hidden layers.
    var layer_defs = [];
    layer_defs.push({type:'input', out_sx:1, out_sy:1, out_depth:network_size});
    layer_defs.push({type:'fc', num_neurons: 50, activation:'relu'});
    layer_defs.push({type:'fc', num_neurons: 50, activation:'relu'});
    layer_defs.push({type:'regression', num_neurons:num_actions});

    // options for the Temporal Difference learner that trains the above net
    // by backpropping the temporal difference learning rule.
    var tdtrainer_options = {learning_rate:0.001, momentum:0.0, batch_size:64, l2_decay:0.01};

    var opt = {};
    opt.temporal_window = temporal_window;
    opt.experience_size = 30000;
    opt.start_learn_threshold = 1000;
    opt.learning_steps_total = 200000;
    opt.learning_steps_burnin = 3000;
    //opt.gamma = 0.7;
    opt.gamma = gamma;
    // opt.epsilon_min = 0.05;
    opt.epsilon_min = emin;
    opt.epsilon_test_time = 0.05;
    opt.layer_defs = layer_defs;
    opt.tdtrainer_options = tdtrainer_options;
    var brain = new deepqlearn.Brain(num_inputs, num_actions, opt); 
// woohoo

    // line intersection helper function: does line segment (p1,p2) intersect segment (p3,p4) ?
    var line_intersect = function(p1,p2,p3,p4) {
      var denom = (p4.y-p3.y)*(p2.x-p1.x)-(p4.x-p3.x)*(p2.y-p1.y);
      if(denom===0.0) { return false; } // parallel lines
      var ua = ((p4.x-p3.x)*(p1.y-p3.y)-(p4.y-p3.y)*(p1.x-p3.x))/denom;
      var ub = ((p2.x-p1.x)*(p1.y-p3.y)-(p2.y-p1.y)*(p1.x-p3.x))/denom;
      if(ua>0.0&&ua<1.0&&ub>0.0&&ub<1.0) {
        var up = new Vec(p1.x+ua*(p2.x-p1.x), p1.y+ua*(p2.y-p1.y));
        return {ua:ua, ub:ub, up:up}; // up is intersection point
      }
      return false;
    }
    
    var line_point_intersect = function(p1,p2,p0,rad) {
    //  p1 = p1 + 8  //account for png
     // p2 = p2 + 8  //account for png
      var v = new Vec(p2.y-p1.y,-(p2.x-p1.x)); // perpendicular vector
      var d = Math.abs((p2.x-p1.x)*(p1.y-p0.y)-(p1.x-p0.x)*(p2.y-p1.y));
      d = d / v.length();
      if(d > rad) { return false; }
      
      v.normalize();
      v.scale(d);
      var up = p0.add(v);
      if(Math.abs(p2.x-p1.x)>Math.abs(p2.y-p1.y)) {
        var ua = (up.x - p1.x) / (p2.x - p1.x);
      } else {
        var ua = (up.y - p1.y) / (p2.y - p1.y);
      }
      if(ua>0.0&&ua<1.0) {
        return {ua:ua, up:up};
      }
      return false;
    }
    
    // Wall is made up of two points
    var Wall = function(p1, p2) {
      this.p1 = p1;
      this.p2 = p2;
    }
    
    // World object contains many agents and walls and food and stuff
    var util_add_box = function(lst, x, y, w, h) {
      lst.push(new Wall(new Vec(x,y), new Vec(x+w,y)));
      lst.push(new Wall(new Vec(x+w,y), new Vec(x+w,y+h)));
      lst.push(new Wall(new Vec(x+w,y+h), new Vec(x,y+h)));
      lst.push(new Wall(new Vec(x,y+h), new Vec(x,y)));
    }
    
    // item is circle thing on the floor that agent can interact with (see or eat, etc)
    var Item = function(x, y, type) {
      this.p = new Vec(x, y); // position
      this.type = type;
      this.rad = 8; // default radius
      this.age = 0;
      this.cleanup_ = false;
    }
    

    var World = function() {
      this.agents = [];
      this.W = canvas.width;
      this.H = canvas.height;
      
      this.clock = 0;
      
      // set up walls in the world
      //lst, x, y, w, h)
      this.walls = []; 
      var pad = 10;
      util_add_box(this.walls, pad, pad, this.W-pad*2, this.H-pad*2);
      util_add_box(this.walls, 100, 150, 150, 200); // inner walls
      this.walls.pop();
      util_add_box(this.walls, 425, 100, 200, 300);
      this.walls.pop();
      
      // set up food and poison
      this.items = []
      for(var k=0;k<items_total;k++) {
        var x = randf(20, this.W-20);
        var y = randf(20, this.H-20);
        var t = randi(1, 3); // food or poison (1 and 2)
        var it = new Item(x, y, t);
        this.items.push(it);
      }
    }
    
    World.prototype = {      
      // helper function to get closest colliding walls/items
      stuff_collide_: function(p1, p2, check_walls, check_items) {
        var minres = false;
        
        // collide with walls
        if(check_walls) {
          for(var i=0,n=this.walls.length;i<n;i++) {
            var wall = this.walls[i];
            var res = line_intersect(p1, p2, wall.p1, wall.p2);
            if(res) {
              res.type = 0; // 0 is wall
              if(!minres) { minres=res; }
              else {
                // check if its closer
                if(res.ua < minres.ua) {
                  // if yes replace it
                  minres = res;
                }
              }
            }
          }
        }
        
        // collide with items
        if(check_items) {
          for(var i=0,n=this.items.length;i<n;i++) {
            var it = this.items[i];
            var res = line_point_intersect(p1, p2, it.p, it.rad);
           // console.log(res)
            if(res) {
              res.type = it.type; // store type of item
              if(!minres) { minres=res; }
              else { if(res.ua < minres.ua) { minres = res; }
              }
            }
          }
        }
        
        return minres;
      },
      tick: function() {
        // tick the environment
        this.clock++;
        
        // fix input to all agents based on environment
        // process eyes
        this.collpoints = [];
        for(var i=0,n=this.agents.length;i<n;i++) {
          var a = this.agents[i];
          for(var ei=0,ne=a.eyes.length;ei<ne;ei++) {
            var e = a.eyes[ei];
            // we have a line from p to p->eyep
            var eyep = new Vec(a.p.x + e.max_range * Math.sin(a.angle + e.angle), //changed this //hmm
                               a.p.y + e.max_range * Math.cos(a.angle + e.angle));
            var res = this.stuff_collide_(a.p, eyep, true, true);
            if(res) {
              // eye collided with wall
              e.sensed_proximity = res.up.dist_from(a.p);
              e.sensed_type = res.type;
            } else {
              e.sensed_proximity = e.max_range;
              e.sensed_type = -1;
            }
          }
        }
        
        // let the agents behave in the world based on their input
        for(var i=0,n=this.agents.length;i<n;i++) {
          this.agents[i].forward();
        }
        
        // apply outputs of agents on evironment
        for(var i=0,n=this.agents.length;i<n;i++) {
          var a = this.agents[i];
          a.op = a.p; // back up old position
          a.oangle = a.angle; // and angle
          

          var v = new Vec(0, a.rad / 2.0); //x: 0, y: 5
          v = v.rotate(a.angle + Math.PI/2);  //get v looking at direction of past movement
          var w1p = a.p.add(v); // positions of wheel 1 and 2
          var w2p = a.p.sub(v); // points on before and after last position
          var vv = a.p.sub(w2p); 
          vv = vv.rotate(-(a.rot1));  //a 1 here is a complete rotation
          var vv2 = a.p.sub(w1p);
          vv2 = vv2.rotate(a.rot2);
          var np = w2p.add(vv);
          
          np.scale(0.5);
          var np2 = w1p.add(vv2);
          np2.scale(0.5);
          a.p = np.add(np2);

          a.angle -= a.rot1;
          if(a.angle<0)a.angle+=2*Math.PI;
          a.angle += a.rot2;

          //console.log(a.angle)

          if(a.angle>2*Math.PI)a.angle-=2*Math.PI;
          //0 is straight down and 3.14 is up goes between 0 to 6.28
          // agent is trying to move from p to op. Check walls
          var res = this.stuff_collide_(a.op, a.p, true, false);
          if(res) {
            // wall collision! reset position
            a.p = a.op;
          }
          
          // handle boundary conditions
          if(a.p.x<0)a.p.x=0;
          if(a.p.x>this.W)a.p.x=this.W;
          if(a.p.y<0)a.p.y=0;
          if(a.p.y>this.H)a.p.y=this.H;
        }
        
        // tick all items
        var update_items = false;
        for(var i=0,n=this.items.length;i<n;i++) {
          var it = this.items[i];
          it.age += 1;
          
          // see if some agent gets lunch
          for(var j=0,m=this.agents.length;j<m;j++) {
            var a = this.agents[j];
            var d = a.p.dist_from(it.p);
            if(d < it.rad + a.rad) {
              
              // wait lets just make sure that this isn't through a wall
              var rescheck = this.stuff_collide_(a.p, it.p, true, false);
              if(!rescheck) { 
                // ding! nom nom nom
                if(it.type === 1) a.digestion_signal += dig_water; // mmm delicious food was 5
                if(it.type === 2) a.digestion_signal += dig_poop; // ewww poison was -6
                it.cleanup_ = true;
                update_items = true;
                break; // break out of loop, item was consumed
              }
            }

          //console.log(this.items[i].p)
          it.p.x = it.p.x + xspd //added for x drift - .1 works
          it.p.y = it.p.y + yspd //added for x drift - .1 works

          }
          
          //clean up old food or food that falls off screen
          if((it.age > 5000 && this.clock % 10 === 0 && randf(0,1)<0.1 )| it.p.x > this.W-30 | it.p.y > this.H-33 | it.p.x < 5 | it.p.y <5) {
            it.cleanup_ = true; // replace this one, has been around too long
            update_items = true;
          }
        }
        if(update_items) {
          var nt = [];
          for(var i=0,n=this.items.length;i<n;i++) {
            var it = this.items[i];
            if(!it.cleanup_) nt.push(it);
          }
          this.items = nt; // swap
        }
        //console.log(this.items.length)

        if((this.items.length < items_total && this.clock % 2 === 0 && randf(0,1)<0.25)) {
          var newitx = randf(1, this.W-20-xspd*2);  //multiple by speed
          var newity = randf(1, this.H-20-yspd*2);
          var newitt = randi(1, 3); // food or poison (1 and 2)
          var newit = new Item(newitx, newity, newitt);
          this.items.push(newit);
        }
        // agents are given the opportunity to learn based on feedback of their action on environment
        for(var i=0,n=this.agents.length;i<n;i++) {
          this.agents[i].backward();
        }
      }
    }

    
    // Eye sensor has a maximum range and senses walls
    var Eye = function(angle) {
      this.angle = angle; // angle relative to agent its on
      this.max_range = 90;
      this.sensed_proximity = 90; // what the eye is seeing. will be set in world.tick()
      this.sensed_type = -1; // what does the eye see?
    }
    
    // A single agent
    var Agent = function() {
    
      // positional information
      this.p = new Vec(50, 50);
      this.op = this.p; // old position
      this.angle = 0; // direction facing
      
      this.actions = [];
      this.actions.push([1,1]);  //straight down or forward
      this.actions.push([0.8,1]); //loop counterclockwise
      this.actions.push([1,0.8]); //loop clockwise
      this.actions.push([0.5,0]); //clockwise
      this.actions.push([0,0.5]); //counterclockwise
      
      // properties
      this.rad = 8;
      this.eyes = [];
      for(var k=0;k<9;k++) { this.eyes.push(new Eye((k-3)*0.25)); }
      
      // brain
      //this.brain = new deepqlearn.Brain(this.eyes.length * 3, this.actions.length);
      var spec = document.getElementById('qspec').value;
      eval(spec);
      this.brain = brain;
      
      this.reward_bonus = 0.0;
      this.digestion_signal = 0.0;
      
      // outputs on world
      this.rot1 = 0.0; // rotation speed of 1st wheel
      this.rot2 = 0.0; // rotation speed of 2nd wheel
      
      this.prevactionix = -1;
    }
    Agent.prototype = {
      forward: function() {
        // in forward pass the agent simply behaves in the environment
        // create input to brain
        var num_eyes = this.eyes.length;
        var input_array = new Array(num_eyes * 3);
        for(var i=0;i<num_eyes;i++) {
          var e = this.eyes[i];
          input_array[i*3] = 1.0;
          input_array[i*3+1] = 1.0;
          input_array[i*3+2] = 1.0;
          if(e.sensed_type !== -1) {
            // sensed_type is 0 for wall, 1 for food and 2 for poison.
            // lets do a 1-of-k encoding into the input array
            input_array[i*3 + e.sensed_type] = e.sensed_proximity/e.max_range;
            //console.log(input_array) // normalize to [0,1]
          }
        }

        // get action from brain
        var actionix = this.brain.forward(input_array);  //index
        //console.log(actionix);        
        var action = this.actions[actionix];  //actual input
        //console.log(action);
        this.actionix = actionix; //back this up
        
        // demultiplex into behavior variables
        this.rot1 = action[0]*1;
        this.rot2 = action[1]*1;
        
      },
      backward: function() {
        // in backward pass agent learns.
        // compute reward 
        var proximity_reward = 0.0;
        var num_eyes = this.eyes.length;
        for(var i=0;i<num_eyes;i++) {
          var e = this.eyes[i];
          // agents dont like to see walls, especially up close
          proximity_reward += e.sensed_type === 0 ? e.sensed_proximity/e.max_range : reward_aw;  //avoid wall reward

        }

        proximity_reward = proximity_reward/(num_eyes);

        proximity_reward = Math.min(reward_aw, proximity_reward * 2);  //avoiding wall
        //console.log(proximity_reward);    // agents like to go straight forward
        var forward_reward = 0.0;
        if(this.actionix === 0 && proximity_reward > 0.75) forward_reward = reward_f * proximity_reward;  //forward reward
        
        // agents like to eat good things
        var digestion_reward = this.digestion_signal;
        this.digestion_signal = 0.0;
        
        var reward = proximity_reward + forward_reward + digestion_reward;

        // pass to brain for learning
        this.brain.backward(reward);
      }
    }
    

    var reward_graph = new cnnvis.Graph();
    function draw_stats() {
      var canvas = document.getElementById("vis_canvas");
      var ctx = canvas.getContext("2d");
      var W = canvas.width;
      var H = canvas.height;
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      var a = w.agents[0];
      var b = a.brain;
      var netin = b.last_input_array;
      
      if(w.clock % 200 === 0) {
        reward_graph.add(w.clock/200, b.average_reward_window.get_average());
        var gcanvas = document.getElementById("graph_canvas");
        reward_graph.drawSelf(gcanvas);
      }
    }
    
    // Draw everything
    function draw() {  
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.lineWidth = 1;
      var agents = w.agents;
      
      // draw walls in environment
      ctx.strokeStyle = "rgb(0,0,0)";
      ctx.beginPath();
      for(var i=0,n=w.walls.length;i<n;i++) {
        var q = w.walls[i];
        ctx.moveTo(q.p1.x, q.p1.y);
        ctx.lineTo(q.p2.x, q.p2.y);
      }
      ctx.stroke();
      
      for(var i=0,n=agents.length;i<n;i++) {
        var a = agents[i];
        
        // draw agents body
        ctx.beginPath();
        ctx.drawImage(eater,a.op.x-8,a.op.y-8);
        
        // draw agents sight
        for(var ei=0,ne=a.eyes.length;ei<ne;ei++) {
          var e = a.eyes[ei];
          var sr = e.sensed_proximity;
          if(e.sensed_type === -1 || e.sensed_type === 0) { 
            ctx.strokeStyle = "rgb(60,60,60)"; // wall or nothing //color of the antenna
          }
          if(e.sensed_type === 2) { ctx.strokeStyle = "rgb(179,58,58)"; } // red poison
          if(e.sensed_type === 1) { ctx.strokeStyle = "rgb(255,165,0)"; } // green apple - orange
          ctx.beginPath();
          ctx.lineWidth=1;
          ctx.moveTo(a.op.x + 0, a.op.y + 0);
          ctx.lineTo(a.op.x + 0 + sr * Math.sin(a.oangle + e.angle),
                     a.op.y + 0 + sr * Math.cos(a.oangle + e.angle));
          ctx.stroke();
        }
      }
      
      // draw items
      for(var i=0,n=w.items.length;i<n;i++) {
        var it = w.items[i];
        ctx.beginPath();
        if(it.type === 2) { ctx.drawImage(poop,it.p.x,it.p.y) } // red poison
        if(it.type === 1) { ctx.drawImage(watermelon,it.p.x,it.p.y) } // green apple - orange
      }
      
      w.agents[0].brain.visSelf(document.getElementById('brain_info_div'));
    }
    
    // Tick the world
    function tick() {
      w.tick();
      if(!skipdraw || w.clock % 50 === 0) {
        draw();
        draw_stats();
      }
    }
    

    var simspeed = 2;
    function goveryfast() {
      window.clearInterval(current_interval_id);
      current_interval_id = setInterval(tick, 0);
      skipdraw = true;
      simspeed = 3;
    }
    function gofast() {
      window.clearInterval(current_interval_id);
      current_interval_id = setInterval(tick, 0);
      skipdraw = false;
      simspeed = 2;
    }
    function gonormal() {
      window.clearInterval(current_interval_id);
      current_interval_id = setInterval(tick, 30);
      skipdraw = false;
      simspeed = 1;
    }
    function goslow() {
      window.clearInterval(current_interval_id);
      current_interval_id = setInterval(tick, 200);
      skipdraw = false;
      simspeed = 0;
    }
    function superslow() {
      window.clearInterval(current_interval_id);
      current_interval_id = setInterval(tick, 800);
      skipdraw = false;
      simspeed = 0;
    }

    function savenet() {
      var j = w.agents[0].brain.value_net.toJSON();
      var t = JSON.stringify(j);
      document.getElementById('tt').value = t;
    }
    
    function loadnet() {
      var t = document.getElementById('tt').value;
      var j = JSON.parse(t);
      w.agents[0].brain.value_net.fromJSON(j);
      stoplearn(); // also stop learning
      gonormal();
    }
    
    function startlearn() {
      w.agents[0].brain.learning = true;
    }
    function stoplearn() {
      w.agents[0].brain.learning = false;
    }
    
    function reload() {
      w = new World();
      w.agents = [new Agent()]; // this should simply work. I think... ;\
      reward_graph = new cnnvis.Graph(); // reinit
    }
    
    var w; // global world object
    var current_interval_id;
    var skipdraw = false;
    function start() {
      canvas = document.getElementById("canvas");
      ctx = canvas.getContext("2d");
      w = new World();
      w.agents = [new Agent()];
      gofast();
    }

  </script>

  <style type="text/css">
      canvas { border: 1px solid white; }
    </style>



 </head>
 <body onload="start();">
   <div id="wrap">
   <h1>State Visualizations</h1>
   
   <div>Average reward over time (this should go up as agent becomes better on average at collecting rewards)</div>
   <canvas id="vis_canvas" width="350" height="10"></canvas>
   <canvas id="graph_canvas" width="800" height="200"></canvas><br>

   <div style="font-size:13px;">

   </div>
   <canvas id="canvas" width="800" height="500"></canvas>
   <div id="brain_info_div"></div>
   
   <h1>Controls</h1>
  <button onclick="goveryfast()">Go very fast</button>
   <button onclick="gofast()">Go fast</button>
   <button onclick="gonormal()">Go normal speed</button>
   <button onclick="goslow()">Go slow</button>
  <button onclick="superslow()">Super Slow</button>  <br> 
   <button onclick="startlearn()">Start Learning</button>
   <button onclick="stoplearn()">Stop Learning</button>
    <br> <br>
    <label for="yspeed">Watermelon</label>
    <output name="Waterout" id="rangeoutW">5</output>
    <input type="range" onchange="myRangeW(this.value)" id="rangeW" min="-20" max="20" oninput="rangeoutW.value = rangeW.value">

    <label for="yspeed">Poop</label>
    <output name="Grangeout" id="rangeoutP">-6</output>
    <input type="range" onchange="myRangeP(this.value)" id="rangeP" min="-20" max="20" oninput="rangeoutP.value = rangeP.value">
    <br>
    <label for="yspeed">Avoid Wall Reward</label>
    <output name="Avoidout" id="rangeoutAW">1</output>
    <input type="range" onchange="myRangeAW(this.value)" id="rangeAW" min="-1" max="10" oninput="rangeoutAW.value = rangeAW.value">

    <label for="yspeed">Forward Reward</label>
    <output name="Forwardout" id="rangeoutF">1</output>
    <input type="range" onchange="myRangeF(this.value)" id="rangeF" min="-10" max="10" oninput="rangeoutF.value = rangeF.value">

  <br><br>
    <label for="xspeed">X Food Speed</label>
    <output name="Xrangeout" id="rangeoutX">0</output>
    <input type="range" onchange="myRangeX(this.value)" id="rangeX" min="-100" max="100" oninput="rangeoutX.value = rangeX.value">

    <label for="yspeed">Y Food Speed</label>
    <output name="Yrangeout" id="rangeoutY">0</output>
    <input type="range" onchange="myRangeY(this.value)" id="rangeY" min="-100" max="100" oninput="rangeoutY.value = rangeY.value">

   <br><br>
    <label for="yspeed">Epsilon parameter</label>
    <output name="Erangeout" id="rangeoutE">5</output>
    <input type="range" onchange="myRangeE(this.value)" id="rangeE" min="1" max="100"  oninput="rangeoutE.value = rangeE.value">

    <label for="yspeed">Gamma parameter</label>
    <output name="Grangeout" id="rangeoutG">70</output>
    <input type="range" onchange="myRangeG(this.value)" id="rangeG" min="1" max="100" oninput="rangeoutG.value = rangeG.value">
    <br>

<br>
   <button onclick="reload()" style="width: 100px; height: 30px; margin-top: 5px;margin-bottom: 5px;">Reload Model</button>
      <textarea id="qspec" style="width:100%; height:100px;">
          var layer_defs = [];
          var tdtrainer_options = {learning_rate:0.001, momentum:0.0, batch_size:64, l2_decay:0.01};
          opt.tdtrainer_options = tdtrainer_options;
          items_total = 40;
          opt.epsilon_min = emin;
          opt.gamma = gamma;
          var brain = new deepqlearn.Brain(num_inputs, num_actions, opt); 
   </textarea>
   </div>
</body></html>
